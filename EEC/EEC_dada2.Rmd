---
title: "EEC_DADA2"
output: html_document
---

DADA2 processing of data for eastern English Channel study.

This R-markdown document includes a pipeline for data processing of amplicon data taken from the publication by [link](https://doi. org/10.1371/journal.pone.0196987).

### Version of all the programs and tools used n this R markdown. 

**R** = 3.6.1
**R studio** = 1.2.1335
**dada2** = 1.12.1

### Downloading raw sequence data from the database. 

```{engine='bash'}

# Download sequnce data collected from Feb 2013 to July 2015 
# This datasets were pair-end fastq files. Therefore, the fastq-dump option '--split-files' was used to separate forward and reverse reads. 
./fastq_download.txt SRR_Feb2013_July2015.txt

# Download sequence data collected from March 2011 to July 2013
# This datasets were single end fastq files. Therefore, the fastq-dump option '--split-3' was used to output a single fastq file which is not yet demultiplexed.  
./fastq_download.txt SRR_Mar2011_July2013


```

### Demultiplexing the fastq file from Mar 2011 to July 2013









### Loading the neccesary packages

```{r}
library("knitr")
library("dada2")
```

### Specifying the path

```{r}
<<<<<<< HEAD
# Path for 2011-2013 dataset
path_EEC_2011_2013 <- "C:\\Users\\MSI\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\matomo\\jacobs\\BCCB_Thesis\\EEC\\fastq\\2011-2013_fastq"

# Path for 2013-2015 dataset
path_EEC_2013_2015 <- "C:\\Users\\MSI\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\matomo\\jacobs\\BCCB_Thesis\\EEC\\fastq\\2013-2015"

list.files(path_EEC_2011_2013)
list.files(path_EEC_2013_2015)
=======
path <- "C:\\Users\\MSI\\AppData\\Local\\Packages\\CanonicalGroupLimited.UbuntuonWindows_79rhkp1fndgsc\\LocalState\\rootfs\\home\\matomo\\jacobs\\BCCB_Thesis\\PeakPOC\\peakpoc_fastq\\Original"

list.files(path)
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```

The directory for 50 fastq files is specified. 

### specifying the forward and reverse fastq files
String manipulation to get matched lists of the forward and reverse fastq files
```{r}
<<<<<<< HEAD
# The format of forward files for 2011 - 2013 is specified
fnFs_2011_2013 <- sort(list.files(path_EEC_2011_2013, pattern = ".flow.txt.fastq", full.names = TRUE))

# The format of forward and reverse files for 2013 - 2015 is specified
fnFs_2013_2015 <- sort(list.files(path_EEC_2013_2015, pattern = "_1.fastq", full.names = TRUE))
fnRs_2013_2015 <- sort(list.files(path_EEC_2013_2015, pattern = "_2.fastq", full.names = TRUE))

# Extract sample names for both 2011-2013 and 2013-2015
sample.names_2011_2013 <- sapply(strsplit(basename(fnFs_2011_2013), ".flow"), `[`, 1)
sample.names_2013_2015 <- sapply(strsplit(basename(fnFs_2013_2015), "_"), `[`, 1)
=======
# The format of forward and reverse fastq file name is SAMPLE_NAME_R1_001.fastq and Sample_NAME_R2_00.fastq
fnFs <- sort(list.files(path, pattern = "_R1_001.fastq", full.names = TRUE))
fnRs <- sort(list.files(path, pattern = "_R2_001.fastq", full.names = TRUE))

# Extract sample names, assuming files have format: SAMPLENAME_XXX.fastq
sample.names <- sapply(strsplit(basename(fnFs), "_L"), `[`, 1)
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```

### Visualizing the quality of the fastq files before filtering and trimming

```{r}
<<<<<<< HEAD
# Plot for 2011-2013
plotQualityProfile(fnFs_2011_2013[1:4])

#plot for 2013 -2015
plotQualityProfile(fnFs_2013_2015[1:2])
plotQualityProfile(fnRs_2013_2015[1:2])
=======
plotQualityProfile(fnFs[1:5])
plotQualityProfile(fnRs[1:2])
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```

As shown in the plot graph above, the quality of reads significantly declines at the position of 200 for the forward files and this would be the cutoff point for the truncation. Any point after 200 shows a significant decline in quality score which is not suitable for further processing.   

For reverse file, truncate position is determined at 150.
**fnFs[1:2]** = selecting first two files from the targeted fastq files in the directory

### -Assigning a variable for fastq file names after filtering and trimming.- 

```{r}
<<<<<<< HEAD
# Place filtered files in filtered subdirectory 2011-2013
filtFs_2011_2013 <- file.path(path_EEC_2011_2013, "filtered", paste0(sample.names_2011_2013, "_F_filt.fastq.gz"))
names(filtFs_2011_2013) <- sample.names_2011_2013

# 2013-2015
filtFs_2013_2015 <- file.path(path_EEC_2013_2015, "filtered", paste0(sample.names_2013_2015, "_F_filt.fastq.gz"))
names(filtFs_2013_2015) <- sample.names_2013_2015

# Reverse files
filtRs_2013_2015 <- file.path(path_EEC_2013_2015, "filtered", paste0(sample.names_2013_2015, "_R_filt.fastq.gz"))
names(filtRs_2013_2015) <- sample.names_2013_2015
=======
# Place filtered files in filtered subdirectory
filtFs <- file.path(path, "filtered", paste0(sample.names, "L001_F_filt.fastq.gz"))
names(filtFs) <- sample.names

# Reverse files
filtRs <- file.path(path, "filtered", paste0(sample.names, "L001_R_filt.fastq.gz"))
names(filtRs) <- sample.names
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```

### - filtering and trimming- 
discard low quality reads and primers. 
 
```{r}
<<<<<<< HEAD
#2011 - 2013
out_2011_2013 <- filterAndTrim(
  fwd = fnFs_2011_2013,
  filt = filtFs_2011_2013, 
  truncLen=c(300),
  trimLeft = c(17),
  maxN=0,
  maxEE=c(2),
  truncQ=2,
  rm.phix=TRUE,
  compress=TRUE,
  multithread=TRUE,
  maxLen = 600)
out_2011_2013
```

```{r}
# 2013 - 2015
out_2013_2015 <- filterAndTrim(
  fwd = fnFs_2013_2015,
  filt = filtFs_2013_2015, 
  rev = fnRs_2013_2015,
  filt.rev = filtRs_2013_2015,
  truncLen=c(250,200),
  trimLeft = c(17, 17),
  maxN=0,
  maxEE=c(1, 3),
=======
out <- filterAndTrim(
  fwd = fnFs,
  filt = filtFs, 
  rev = fnRs,
  filt.rev = filtRs,
  truncLen=c(250,200),
  trimLeft = c(17, 18),
  maxN=0,
  maxEE=c(1),
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
  truncQ=2,
  rm.phix=TRUE,
  compress=TRUE,
  multithread=TRUE)
<<<<<<< HEAD

head(out_2013_2015)
=======
head(out)
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```

- **fwd** = The path to the input fastq files. In this case, 'fnFs'.
- **filt** = The path to the output filtered files from fwd. In this case 'filtFs'.
- **trunclen** = Legth of reads after truncation of bases. As shown above, the value for trunclen is decided at 250. 
- **maxN** = After truncation, sequences with more than maxN Ns will be discarded. Since DADA2 requires no Ns, we will stick with default value 0.
- **trimLeft** = The number of nucleotides to remove from the start of each read. If both truncLen and trimLeft are provided, filtered reads will have length truncLen-trimLeft. THis parameter is used when the fastq files still contain primer
- **maxEE** = After truncation, reads with higher than maxEE "expected errors" will be discarded. The maxEE parameter sets the maximum number of "expected errors" allowed in a read. In other words, we want to throw the read away if the read has is likely to have more than value 'maxEE' erroneous base calls. The EE is defined to be the mean of errors that would be observed in a very large collection of sequences where error rate in each read position is occured independently. Expected error is the sum of error probabilities. For instance, 
EE = sum(Probability of an error; the base is incorrect if P is the error probability) = sum(10^(-Q/10)). 
If P = 0.5 that means there is a 50% of chance that the base is wrong. Therefore, large EE number implies that the sum of probabilities of error is large as well, so if maxEE is set to low, then only the reads with small sum of error probabilities can pass through the filter (high quality reads). This time maxEE is set to 1 for the sake of high quality reads filteration and efficiency of computation to process such a large datasets. If filtered reads are too few, then please increase the maxEE value (relaxation of filter). 
- **truncQ** = Truncate reads at the first instance of a quality score less than or equal to truncQ. Default is 2 meaning that reads with quality score of 2 (p error = 0.63096) are automatically truncated since there is a 63% chance of the base being wrong. 
- **rm.phix** = If TRUE, discard reads that match against the phiX genome. Phix bacteriophage genome is typically added to illumina sequencing runs for quality monitoring. 
- **compress** = If TRUE, the output fastq files are gzipped
- **multithread** = if TRUE, input files are filtered in parallel via mclapply. It allows it paralell computation which results in faster processing time. 

### Quaity check after filtering and trimming

```{r}
<<<<<<< HEAD
plotQualityProfile(filtFs_2011_2013[1:2])
plotQualityProfile(filtFs_2013_2015[1:2])
=======
plotQualityProfile(filtFs[1:2])
plotQualityProfile(filtRs[1:2])
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```


The quality seems to be good! 

### -learn the error rates -
Every amplicon dataset has a different set of error rates. 
Blackline shows the estimated error rates after convergence of the machine-learning algorithm.
Redline shows the error rates expected under the nominal definition of the Q-score.
Black lines are a good fit fot the observed rates and the errror rates drop with increased quality as expected.

```{r}  
<<<<<<< HEAD
#2011-2013
errF_2011_2013 <- learnErrors(filtFs_2011_2013, multithread=TRUE)

#2013-2015
errF_2013_2015 <- learnErrors(filtFs_2013_2015, multithread=TRUE)
errR_2013_2015 <- learnErrors(filtRs_2013_2015, multithread = TRUE)

plotErrors(errF_2011_2013, nominalQ=TRUE)

plotErrors(errF_2013_2015, nominalQ=TRUE) 
=======
errF <- learnErrors(filtFs, multithread=TRUE)
errR <- learnErrors(filtRs, multithread = TRUE)
plotErrors(errF, nominalQ=TRUE) 
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```
- **multithread** = If TRUE, multithreading is enabled and the number of availble thread is automatically determined. Just like above, this parameter is set to TRUE for faster computation. 
- **nominalQ** =  If TRUE, plot the expected error rates (red line shown in the graph) if quality scores exactly matched their nominal definition: Q = -10 log10(p_err).

The red line is expected line based on the given quality score, the black line indicates the estimated line, and the black dots shows the observed error frequency in each consensus quality score. Ideally, the black dots should follow the track of the black line. 
From the graph above, the black dots follow the trend of black line, and the error rates drop with increased quality as expected. This is reasonable error inference. 

### -Dereplication combines all identical sequenceing reads into "unique sequence" witha corresponding "abundance"-
```{r}
# forward files
<<<<<<< HEAD
derep_forward_2011_2013 <- derepFastq(filtFs_2011_2013, verbose=TRUE)
derep_forward_2013_2015 <- derepFastq(filtFs_2013_2015, verbose=TRUE)

# Name the derep-class object by the sample names. 
names(derep_forward_2011_2013) <- sample.names_2011_2013
names(derep_forward_2013_2015) <- sample.names_2013_2015

# reverse files
derep_reverse_2013_2015 <- derepFastq(filtRs_2013_2015, verbose = TRUE)
names(derep_reverse_2013_2015) <- sample.names_2013_2015
=======
derep_forward <- derepFastq(filtFs, verbose=TRUE)
# Name the derep-class object by the sample names. 
names(derep_forward) <- sample.names

# reverse files
derep_reverse <- derepFastq(filtRs, verbose = TRUE)
names(derep_reverse) <- sample.names
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f

```
- **verbose** = if TRUE, throw standardR messengeson the intermittent and final status of the dereplication. In this case, it is set to TRUE so that the process of dereplication is show in the intermittent. 

### Sample Inference
This process contains removal of unique sequences that were produced by error. 

```{r}
<<<<<<< HEAD
# 2011-2013
dadaFs_2011_2013 <- dada(derep_forward_2011_2013, err=errF_2011_2013, HOMOPOLYMER_GAP_PENALTY = -1, BAND_SIZE=32, pool = "pseudo")
# 2013-2015
dadaFs_2013_2015 <- dada(derep_forward_2013_2015, err=errF_2013_2015, multithread=TRUE, pool="pseudo")
dadaRs_2013_2015 <- dada(derep_reverse_2013_2015, err=errR_2013_2015, multithread=TRUE, pool="pseudo")
dadaFs_2011_2013[[1]]
dadaFs_2013_2015[[1]]
=======
dadaFs <- dada(derep_forward, err=errF, multithread=TRUE, pool="pseudo")
dadaRs <- dada(derep_reverse, err=errR, multithread=TRUE, pool="pseudo")
dadaFs[[1]]
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```
- **err** = 16xN numeric matrix, or an object coercible by getErrors such as the output of the learnErrors function operated in the previous step. 'errF' is the name of variable that was used to store the result from the previous step.
- **pool** = If pool = TRUE, the algorithm will pool together all samples prior to sample inference. If pool = FALSE, sample inference is performed on each sample individually. If pool = "pseudo", the algorithm will perform pseudo-pooling between individually processed samples. In other words, when sample A has 1000 copies of of sequence Z while sample B only contains one single copy of sequence Z, sequence Z is likely to be filtered out of sample B although it was a true "singleton" among other sequences in the sample B. This is what is expected to happen when the parameter 'pool' is set to FALSE. On the other hand, when the paramter 'pool' is set to TRUE, it is going to require inconvinient over-workload of computation if the large datasets are processed. For this reason, the parameter "pseudo" option lies somewhere between these two pooling options. Pesudo option contains a two step process i which independent processing is performed twice: First on the raw data alone, and then on the raw data again but informed by priors generated from the first round of processing in the second time. Pseudo-pooling provides a more accurate resolution of ASVs. For this reason, pseudo option is used in this datasets. 

From the first sample, 132 true sequence variants from the 2967 unique sequence are detected. 


<<<<<<< HEAD
### Merging forward and reverse reads together for 2013 - 2015

```{r}
mergers_2013_2015 <- mergePairs(dadaFs_2013_2015, derep_forward_2013_2015, dadaRs_2013_2015, derep_reverse_2013_2015, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers_2013_2015[1:5])
=======
### Merging forward and reverse reads together. 

```{r}
mergers <- mergePairs(dadaFs, derep_forward, dadaRs, derep_reverse, verbose=TRUE)
# Inspect the merger data.frame from the first sample
head(mergers[1:5])
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```

Now the forward and the reverse files are merged. 

### - Construction of ASV files- 

```{r}
<<<<<<< HEAD
# 2011-2013
seqtab_2011_2013 <- makeSequenceTable(samples = dadaFs_2011_2013)
dim(seqtab_2011_2013)
# inspect distribution of sequence lengths
table(nchar(getSequences(seqtab_2011_2013)))

# #remove non-target length sequences from the sequence table
# seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 349:451]
# table(nchar(getSequences(seqtab2)))

# 2011-2013
seqtab_2013_2015 <- makeSequenceTable(samples = dadaFs_2013_2015)
dim(seqtab_2013_2015)
# inspect distribution of sequence lengths
table(nchar(getSequences(seqtab_2013_2015)))
=======
seqtab <- makeSequenceTable(samples = mergers)
dim(seqtab)
# inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

#remove non-target length sequences from the sequence table
seqtab2 <- seqtab[,nchar(colnames(seqtab)) %in% 349:451]
table(nchar(getSequences(seqtab2)))
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
``` 

### -Chimera detection- 
```{r}
<<<<<<< HEAD
#2011-2013
seqtab.nochim_2011_2013 <- removeBimeraDenovo(unqs = seqtab_2011_2013, method = "consensus", multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim_2011_2013)
sum(seqtab.nochim_2011_2013)/sum(seqtab_2011_2013)

#2013-2015
seqtab.nochim_2013_2015 <- removeBimeraDenovo(unqs = seqtab_2013_2015, method = "consensus", multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim_2013_2015)
sum(seqtab.nochim_2013_2015)/sum(seqtab_2013_2015)

=======
seqtab.nochim <- removeBimeraDenovo(unqs = seqtab2, method = "consensus", multithread = TRUE, verbose = TRUE)
dim(seqtab.nochim)
sum(seqtab.nochim)/sum(seqtab)
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
```
Chimera is an errous biological sequence derived from two parent sequences during the process of amplification 
- **unqs** = Object that can be coerced in to one with getUniques. In this case seqtab from the previous step.
- **method** = If "pooled": The samples in the sequence table are all pooled together for bimera identification. If "consensus": The samples in a sequence table are independently checked for bimeras, and a consensus decision on each sequence variant is made. If "per-sample": The samples in a sequence table are independently checked for bimeras, and sequence variants are removed (zeroed-out) from samples independently.
- **verbose** = print verbose text output if TRUE. 

The table contains 1572 ASVs from 25 samples. 

only 2 % of the sequences were chimeras. 

### -Tracking the reads from the pipeline-
```{r}
<<<<<<< HEAD
#2011-2013
getN <- function(x) sum(getUniques(x))
track_2011_2013 <- cbind(out_2011_2013, sapply(dadaFs_2011_2013, getN), rowSums(seqtab.nochim_2011_2013))
# if processing a single sample, remove the sapply calls
colnames(track_2011_2013) <- c("input", "filtered", "denoisedF", "nonchim")
rownames(track_2011_2013) <- sample.names_2011_2013
head(track_2011_2013)

#2013-2015
track_2013_2015 <- cbind(out_2013_2015, sapply(dadaFs_2013_2015, getN),sapply(dadaRs_2013_2015, getN), rowSums(seqtab.nochim_2013_2015))
# if processing a single sample, remove the sapply calls
colnames(track_2013_2015) <- c("input", "filtered", "denoisedF", "denoisedR", "nonchim")
rownames(track_2013_2015) <- sample.names_2013_2015
head(track_2013_2015)
```


### exporting files
```{r}

#2011-2013
# Giving our seq headers more manageable names
asv_seqs_2011_2013 <- colnames(seqtab.nochim_2011_2013)
asv_headers_2011_2013 <- vector(dim(seqtab.nochim_2011_2013)[2], mode="character")
for (i in 1:dim(seqtab.nochim_2011_2013)[2]) {
  asv_headers_2011_2013[i] <- paste(">ASV", i, seq="_")
}

# making and writing out a fast of our final ASV seqs:
asv_fasta_2011_2013 <- c(rbind(asv_headers_2011_2013, asv_seqs_2011_2013))
# File name can be changed by modifying "ASVs.fa"
write(asv_fasta_2011_2013, "ASVs_EEC_2011_2013.fa") 


# ASV table
asv_tab_2011_2013 <- t(seqtab.nochim_2011_2013)
row.names(asv_tab_2011_2013) <- sub(">", "", asv_headers_2011_2013)
# File name can be changed by modifying "ASVs_counts.tsv"
write.table(asv_tab_2011_2013, "ASVs_EEC_2011_2013.tsv", sep="\t", quote=F, col.names=NA)


#2013-2015
### exporting files

# Giving our seq headers more manageable names
asv_seqs_2013_2015 <- colnames(seqtab.nochim_2013_2015)
asv_headers_2013_2015 <- vector(dim(seqtab.nochim_2013_2015)[2], mode="character")
for (i in 1:dim(seqtab.nochim_2013_2015)[2]) {
  asv_headers_2013_2015[i] <- paste(">ASV", i, seq="_")
}

# making and writing out a fast of our final ASV seqs:
asv_fasta_2013_2015 <- c(rbind(asv_headers_2013_2015, asv_seqs_2013_2015))
# File name can be changed by modifying "ASVs.fa"
write(asv_fasta_2013_2015, "ASVs_EEC_2013_2015.fa") 


# ASV table
asv_tab_2013_2015 <- t(seqtab.nochim_2013_2015)
row.names(asv_tab_2013_2015) <- sub(">", "", asv_headers_2013_2015)
# File name can be changed by modifying "ASVs_counts.tsv"
write.table(asv_tab_2013_2015, "ASVs_EEC_2013_2015.tsv", sep="\t", quote=F, col.names=NA)


```


```
=======
getN <- function(x) sum(getUniques(x))
track <- cbind(out, sapply(dadaFs, getN),sapply(dadaRs, getN), sapply(mergers, getN), rowSums(seqtab.nochim))
# if processing a single sample, remove the sapply calls
colnames(track) <- c("input", "filtered", "denoisedF", "denoisedR", "merged", "nonchim")
rownames(track) <- sample.names
head(track)
```
>>>>>>> 14968be03e58402f2db64cc5a4f8fe6274facb3f
